{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import time\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrd\n",
    "import numpy as np\n",
    "\n",
    "from jax.random import PRNGKey\n",
    "from flax import linen as nn\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "from kronojax.neural.KAN import KAN, MLP\n",
    "from kronojax.neural.embedding import time_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random Key\n",
    "key = jrd.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametrize flow and drift functions with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_drift(nn.Module):\n",
    "    \"\"\"parametrize drift function\"\"\"\n",
    "\n",
    "    time_embedding_dim: int\n",
    "    time_freq_min: float\n",
    "    time_freq_max: float\n",
    "    dim_list: list\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "\n",
    "        t_embedded = time_embedding(t, self.time_freq_min, self.time_freq_max, self.time_embedding_dim)\n",
    "        input_forward = jnp.concatenate([x, t_embedded], axis=1)\n",
    "        NN_forward = KAN(dim_list=self.dim_list, degree=2)\n",
    "\n",
    "        return NN_forward(input_forward)\n",
    "\n",
    "class NN_flow(nn.Module):\n",
    "    \"\"\"parametrize flow function\"\"\"\n",
    "\n",
    "    time_embedding_dim: int\n",
    "    time_freq_min: float\n",
    "    time_freq_max: float\n",
    "    dim_list: list\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "\n",
    "        t_embedded = time_embedding(t, self.time_freq_min, self.time_freq_max, self.time_embedding_dim)\n",
    "        input_flow = jnp.concatenate([x, t_embedded], axis=1)\n",
    "        NN_flow = MLP(dim_list=self.dim_list)\n",
    "\n",
    "        return jnp.exp(NN_flow(input_flow))\n",
    "    \n",
    "class NN_FlowAndDirft(nn.Module):\n",
    "    \"\"\"concatenate two neural networks for drift and flow\"\"\"\n",
    "\n",
    "    time_embedding_dim: int\n",
    "    time_freq_min: float\n",
    "    time_freq_max: float\n",
    "    dim_list_drift: list\n",
    "    dim_list_flow: list\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "\n",
    "        drift = NN_drift(time_embedding_dim=self.time_embedding_dim, \n",
    "                         time_freq_min=self.time_freq_min, time_freq_max=self.time_freq_max, dim_list=self.dim_list_drift)\n",
    "        \n",
    "        flow = NN_flow(time_embedding_dim=self.time_embedding_dim, \n",
    "                       time_freq_min=self.time_freq_min, time_freq_max=self.time_freq_max, dim_list=self.dim_list_flow)\n",
    "\n",
    "        return drift(x, t), flow(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jrd.split(key)\n",
    "time_embedding_dim = 16\n",
    "time_freq_min = 1.\n",
    "time_freq_max = 10\n",
    "output_dim = 1\n",
    "dim_list_drift = [32, 32, output_dim]\n",
    "dim_list_flow = [32, 32, output_dim]\n",
    "GFnet = NN_FlowAndDirft(time_embedding_dim, time_freq_min, time_freq_max, dim_list_drift, dim_list_flow)\n",
    "# net_drift = NN_drift(time_embedding_dim, time_freq_min, time_freq_max, dim_list_drift)\n",
    "# net_flow = NN_flow(time_embedding_dim, time_freq_min, time_freq_max, dim_list_flow)\n",
    "\n",
    "# initialize the network\n",
    "batch_sz = 32\n",
    "key, subkey = jrd.split(key)\n",
    "xs = jrd.normal(subkey, (batch_sz, 1))\n",
    "ts = jrd.uniform(subkey, (batch_sz, 1))\n",
    "params = GFnet.init(subkey, xs, ts)\n",
    "# params_drift = net_drift.init(subkey, xs, ts)\n",
    "# params_flow = net_flow.init(subkey, xs, ts)\n",
    "# params = {'drift': params_drift, 'flow': params_flow}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(params_drift['params']['KAN_0']['ChebyKANLayer_0']['cheby_coeffs'].shape)\n",
    "# print(params_drift['params']['KAN_0']['LayerNorm_0']['scale'].shape)\n",
    "# print(params_drift['params']['KAN_0']['LayerNorm_0']['bias'].shape)\n",
    "\n",
    "# print(params_drift['params']['KAN_0']['ChebyKANLayer_1']['cheby_coeffs'].shape)\n",
    "# print(params_drift['params']['KAN_0']['LayerNorm_1']['scale'].shape)\n",
    "# print(params_drift['params']['KAN_0']['LayerNorm_1']['bias'].shape)\n",
    "\n",
    "# print(params_drift['params']['KAN_0']['ChebyKANLayer_2']['cheby_coeffs'].shape)\n",
    "\n",
    "# print(params_flow['params']['MLP_0']['Dense_0']['kernel'].shape)\n",
    "# print(params_flow['params']['MLP_0']['Dense_0']['bias'].shape)\n",
    "\n",
    "# print(params_flow['params']['MLP_0']['Dense_1']['kernel'].shape)\n",
    "# print(params_flow['params']['MLP_0']['Dense_1']['bias'].shape)\n",
    "\n",
    "# print(params_flow['params']['MLP_0']['Dense_2']['kernel'].shape)\n",
    "# print(params_flow['params']['MLP_0']['Dense_2']['bias'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_density(x, mu, sigma2):\n",
    "    \"\"\"\n",
    "    x: n-dim array\n",
    "    mu: n-dim array\n",
    "    sigma2: scalar\n",
    "    \"\"\"\n",
    "    return jnp.exp(-0.5 * (x - mu)**2 / sigma2) / jnp.sqrt(2 * jnp.pi * sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Traj(\n",
    "        params: any,\n",
    "        batch_sz: int,\n",
    "        N_step: int,\n",
    "        key: PRNGKey\n",
    "        ):\n",
    "    \"\"\"keep track on flow function, forward probability, and backward probability\"\"\"\n",
    "\n",
    "    T = 1.\n",
    "    dt = T / N_step\n",
    "    sqt = jnp.sqrt(dt)\n",
    "\n",
    "    def _step(carry, _):\n",
    "        xo, t = carry\n",
    "        to = jnp.ones_like(xo) * t\n",
    "        # u = net_drift.apply(params_drift, xo, to)\n",
    "        # f = net_flow.apply(params_flow, xo, to)\n",
    "        u, f = GFnet.apply(params, xo, to)\n",
    "        dw = sqt * jrd.normal(subkey, xo.shape)\n",
    "        xn = xo + u*dt + dw\n",
    "        t += dt\n",
    "        PF = normal_density(xn, xo + u*dt, dt) # Forward probability\n",
    "        PB = normal_density(xo, xn, dt) # Backward probability\n",
    "        output_dict = {\n",
    "            \"t\": t,\n",
    "            \"x\": xo,\n",
    "            \"P_forward\": PF,\n",
    "            \"P_backward\": PB,\n",
    "            \"state flow\": f\n",
    "            }\n",
    "        return (xn, t), output_dict\n",
    "    \n",
    "    key, subkey = jrd.split(key)\n",
    "    t_init = 0.\n",
    "    x_init = jnp.zeros((batch_sz, 1))\n",
    "    carry_init = (x_init, t_init)\n",
    "    _, trajectory = jax.lax.scan(_step, carry_init, xs = None, length=N_step)\n",
    "    xT = trajectory[\"x\"][-1]\n",
    "    tT = jnp.ones_like(xT)\n",
    "    # FN = net_flow.apply(params_flow, xT, tT)\n",
    "    _, FN = GFnet.apply(params, xT, tT)\n",
    "    return trajectory, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(10, 32, 1)\n",
      "(10, 32, 1)\n",
      "(10, 32, 1)\n",
      "(10, 32, 1)\n",
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "trajectory, FN= Traj(params, batch_sz, 10, key)\n",
    "print(trajectory[\"t\"].shape)\n",
    "print(trajectory[\"x\"].shape)\n",
    "print(trajectory[\"P_forward\"].shape)\n",
    "print(trajectory[\"P_backward\"].shape)\n",
    "print(trajectory[\"state flow\"].shape)\n",
    "print(FN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "        params, \n",
    "        batch_sz: int, \n",
    "        N_step: int, \n",
    "        key: PRNGKey\n",
    "        ):\n",
    "    \"\"\"loss function -- total trajectory balance\"\"\"\n",
    "    # params_drift = params['drift']\n",
    "    # params_flow = params['flow']\n",
    "    trajectory, FN = Traj(params, batch_sz, N_step, key)\n",
    "    F0 = trajectory[\"state flow\"][0]\n",
    "    ratio = jnp.log(F0 / FN) + jnp.sum(jnp.log(trajectory[\"P_forward\"] / trajectory[\"P_backward\"]), axis=0)\n",
    "    loss = jnp.mean(ratio**2, axis=0)\n",
    "    return loss.reshape(())\n",
    "\n",
    "loss = jax.jit(loss, static_argnums=(1, 2))\n",
    "loss_value_grad = jax.value_and_grad(loss, argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(5.8536606, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(params, 32, 10, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update parameters via loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "optimizer = optax.adam(learning_rate=lr)\n",
    "\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "def update(params, opt_state, batch_sz, N_step, key):\n",
    "    \"\"\"update the parameters\"\"\"\n",
    "\n",
    "    loss_value, grads = loss_value_grad(params, N_step, batch_sz, key)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss_value\n",
    "\n",
    "update = jax.jit(update, static_argnums=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss 150.5684814453125\n",
      "Iteration    0/10  |  Loss: 150.57  |  Time: 1.97 s\n",
      "Iteration 1, Loss nan\n",
      "Iteration 2, Loss nan\n",
      "Iteration 3, Loss nan\n",
      "Iteration 4, Loss nan\n",
      "Iteration 5, Loss nan\n",
      "Iteration 6, Loss nan\n",
      "Iteration 7, Loss nan\n",
      "Iteration 8, Loss nan\n",
      "Iteration 9, Loss nan\n"
     ]
    }
   ],
   "source": [
    "Niter = 10\n",
    "Batch_SZ = 256\n",
    "LR = 10**-2\n",
    "optimizer = optax.adam(learning_rate=LR)\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "time_start = time.time()\n",
    "for i in range(Niter):\n",
    "    key, subkey = jrd.split(key)\n",
    "    params, opt_state, loss_value = update(params, opt_state, Batch_SZ, 10, subkey)\n",
    "    loss_values.append(loss_value)\n",
    "    print(f\"Iteration {i}, Loss {loss_value}\")\n",
    "    if i % 10 == 0:\n",
    "        time_current = time.time()\n",
    "        print(f\"Iteration {i:4d}/{Niter}  |  Loss: {loss_value:.2f}  |  Time: {time_current - time_start:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
